OlÃ¡,  
> Venho compartilhar meu conhecimento em **Reconhecimento de PadrÃµes**, deixando exemplos, explicaÃ§Ãµes e conceitos de fÃ¡cil acesso.  
> Este repositÃ³rio funciona como um portfÃ³lio pessoal para organizaÃ§Ã£o e estudos na Ã¡rea de **aprendizado de mÃ¡quina e inteligÃªncia artificial**.  

# ğŸ¤– Reconhecimento de PadrÃµes

## ğŸ“Œ Ãndice
- [ğŸ¤– Reconhecimento de PadrÃµes](#-reconhecimento-de-padrÃµes)
  - [ğŸ“Œ Ãndice](#-Ã­ndice)
  - [ğŸ“Œ DescriÃ§Ã£o](#-descriÃ§Ã£o)
    - [ğŸ“Š Estimadores de Densidade ProbabilÃ­stica](#-estimadores-de-densidade-probabilÃ­stica)
      - [ğŸ”¹ Estimador Gaussiano](#-estimador-gaussiano)
      - [ğŸ”¹ Kernel Density Estimation (KDE)](#-kernel-density-estimation-kde)
    - [ğŸ“Œ Classificador Bayesiano](#-classificador-bayesiano)
      - [ğŸ“ Teorema de Bayes](#-teorema-de-bayes)
      - [âš™ï¸ Naive Bayes](#ï¸-naive-bayes)
  - [ğŸ“Œ Exemplos de ImplementaÃ§Ã£o](#-exemplos-de-implementaÃ§Ã£o)
  - [ğŸ“Œ Requisitos](#-requisitos)
  - [ğŸ“Œ Uso](#-uso)

---

## ğŸ“Œ DescriÃ§Ã£o  

O projeto aborda conceitos fundamentais de **Reconhecimento de PadrÃµes** utilizando **estimadores de densidade probabilÃ­stica** e **classificadores Bayesianos**.  
O objetivo Ã© aplicar tÃ©cnicas estatÃ­sticas para identificar a qual classe uma amostra pertence, com base em seu vetor de caracterÃ­sticas.

---

### ğŸ“Š Estimadores de Densidade ProbabilÃ­stica  

Um **estimador de densidade** Ã© utilizado para calcular a probabilidade de um dado valor ocorrer, modelando a distribuiÃ§Ã£o de atributos em cada classe.  

#### ğŸ”¹ Estimador Gaussiano  

O **Estimador Gaussiano** assume que os dados seguem uma **distribuiÃ§Ã£o normal**.  
Ele utiliza **mÃ©dia** e **desvio padrÃ£o** para cada atributo de cada classe, aplicando a funÃ§Ã£o de densidade da normal:

\[
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \cdot e^{-\frac{(x - \mu)^2}{2\sigma^2}}
\]

âœ”ï¸ Vantagens: rÃ¡pido e simples.  
âŒ LimitaÃ§Ã£o: funciona bem apenas se os dados forem aproximadamente normais.  

---

#### ğŸ”¹ Kernel Density Estimation (KDE)  

O **KDE** Ã© um estimador **nÃ£o paramÃ©trico**, que nÃ£o assume forma prÃ©via da distribuiÃ§Ã£o.  
Ele constrÃ³i a densidade a partir de **nÃºcleos (kernels)** suavizados sobre cada ponto de treino:

\[
\hat{f}(x) = \frac{1}{n h} \sum_{i=1}^{n} K\left(\frac{x - x_i}{h}\right)
\]

Onde:  
- `K` Ã© a funÃ§Ã£o nÃºcleo (por exemplo, Gaussiana).  
- `h` Ã© a largura de banda (*bandwidth*).  

âœ”ï¸ Vantagens: captura distribuiÃ§Ãµes complexas.  
âŒ LimitaÃ§Ã£o: mais custoso computacionalmente e depende da escolha de `h`.  

---

### ğŸ“Œ Classificador Bayesiano  

Um **Classificador Bayesiano** estima a probabilidade de uma amostra pertencer a uma classe `C` a partir dos atributos `X`, aplicando o **Teorema de Bayes**.  

#### ğŸ“ Teorema de Bayes  

\[
P(C|X) = \frac{P(X|C) \cdot P(C)}{P(X)}
\]

- **P(C)** â†’ Probabilidade a priori da classe (frequÃªncia no treino).  
- **P(X|C)** â†’ Probabilidade da amostra dado a classe (estimada com Gaussiana ou KDE).  
- **P(X)** â†’ Constante de normalizaÃ§Ã£o (mesma para todas as classes).  

---

#### âš™ï¸ Naive Bayes  

O **Naive Bayes** Ã© uma simplificaÃ§Ã£o que assume independÃªncia entre os atributos:  

\[
P(X|C) = \prod_{i=1}^{n} P(X_i | C)
\]

âœ”ï¸ FÃ¡cil de implementar.  
âœ”ï¸ Funciona bem mesmo em cenÃ¡rios simples.  
âŒ Pode perder precisÃ£o quando os atributos sÃ£o fortemente correlacionados.  

---

## ğŸ“Œ Exemplos de ImplementaÃ§Ã£o  

ğŸ”¹ **Gaussiana** â†’ estimativa paramÃ©trica (usa mÃ©dia e desvio).  
ğŸ”¹ **KDE** â†’ estimativa nÃ£o paramÃ©trica (usa todos os pontos do treino).  
ğŸ”¹ **Naive Bayes** â†’ classificador que combina os estimadores para prever a classe mais provÃ¡vel.  

---

## ğŸ“Œ Requisitos  

âœ” **Java JDK 17+**  
âœ” **Biblioteca Swing (para seleÃ§Ã£o de arquivos)**  
âœ” **Arquivos `.data` de treino e teste**  

---

## ğŸ“Œ Uso  

1ï¸âƒ£ Compile o projeto:  
```bash
javac *.java