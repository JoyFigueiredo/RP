# Reconhecimento de Padr√µes

## üìë Sum√°rio
- [Reconhecimento de Padr√µes](#reconhecimento-de-padr√µes)
  - [üìë Sum√°rio](#-sum√°rio)
  - [Explica√ß√£o de Estimadores de Densidade Probabil√≠stica](#explica√ß√£o-de-estimadores-de-densidade-probabil√≠stica)
    - [Estimador Gaussiano](#estimador-gaussiano)
    - [Kernel Density Estimation (KDE)](#kernel-density-estimation-kde)
  - [Classificador Bayesiano](#classificador-bayesiano)
    - [Naive Bayes](#naive-bayes)

---

## Explica√ß√£o de Estimadores de Densidade Probabil√≠stica

Um **estimador de densidade probabil√≠stica** √© usado para modelar como os dados est√£o distribu√≠dos no espa√ßo.  
Esses estimadores permitem calcular a **probabilidade** de uma nova amostra pertencer a uma classe, com base nos dados de treino.

Existem duas abordagens principais implementadas neste projeto:

### Estimador Gaussiano
O estimador gaussiano assume que os dados seguem uma **distribui√ß√£o normal**.  
Ele calcula a **m√©dia** e o **desvio padr√£o** para cada atributo de cada classe e, com isso, obt√©m a fun√ß√£o de densidade de probabilidade:

- ‚úîÔ∏è **Vantagens**: r√°pido, simples, requer apenas m√©dia e vari√¢ncia.  
- ‚ùå **Limita√ß√µes**: funciona bem apenas se os dados realmente tiverem comportamento aproximadamente normal.  

### Kernel Density Estimation (KDE)
O **KDE** √© um estimador **n√£o param√©trico**, ou seja, n√£o assume que os dados seguem uma forma espec√≠fica.  
Em vez disso, ele cria uma estimativa suavizada da densidade ao somar v√°rias fun√ß√µes n√∫cleo (como gaussianas) centradas em cada ponto de treino.

- ‚úîÔ∏è **Vantagens**: flex√≠vel, funciona para distribui√ß√µes complexas ou multimodais.  
- ‚ùå **Limita√ß√µes**: mais lento, depende da escolha da largura de banda (*bandwidth*).  

---

## Classificador Bayesiano

O **Classificador Bayesiano** √© baseado no **Teorema de Bayes**, que calcula a probabilidade de uma amostra pertencer a uma classe `C`, dado um conjunto de atributos `X`.  

Ele se baseia na f√≥rmula:

\[
P(C|X) = \frac{P(X|C) \cdot P(C)}{P(X)}
\]

- **P(C)**: probabilidade a priori da classe (frequ√™ncia da classe no treino).  
- **P(X|C)**: probabilidade de observar os atributos X, dado que a amostra pertence √† classe C.  
- **P(X)**: probabilidade total (constante para todas as classes, pode ser ignorada na pr√°tica).  

### Naive Bayes
Uma simplifica√ß√£o muito utilizada √© o **Naive Bayes**, que assume que todos os atributos s√£o **independentes** entre si.  
Nesse caso:

\[
P(X|C) = \prod_{i=1}^{n} P(X_i | C)
\]

No projeto:  
- O c√°lculo de \( P(X_i | C) \) pode ser feito com **Gaussiana** ou com **KDE**, dependendo do estimador escolhido.  
- A classe final atribu√≠da √© aquela que maximiza \( P(C|X) \).  

---
